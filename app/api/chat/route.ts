// ========================================
// API CHAT - /api/chat
// ========================================
// üìñ Explication simple :
// Cette API g√®re les conversations avec l'assistant RAG.
// Nouvelles fonctionnalit√©s :
// - Cr√©e automatiquement une conversation (Chat) si n√©cessaire
// - Sauvegarde tous les messages dans la base de donn√©es
// - Retourne le chatId pour les messages suivants

import { NextRequest, NextResponse } from "next/server";
import { fileSearchTool, Agent, Runner } from "@openai/agents";
import type { AgentInputItem } from "@openai/agents";
import { prisma } from "@/lib/prisma";

export const runtime = "nodejs";

// Vector Stores
const fileSearch = fileSearchTool([
  "vs_68e87a07ae6c81918d805c8251526bda",
]);

// Mod√®le
const MODEL = process.env.OPENAI_MODEL ?? "gpt-4.1-mini";

const agent = new Agent({
  name: "Agent Endobiog√©nie",
  instructions: `Tu es EndoBot, une intelligence experte en th√©orie de l'endobiog√©nie et en phytoth√©rapie clinique int√©grative.
R√©ponds UNIQUEMENT √† partir des extraits retrouv√©s via File search (Vector Stores).
Si aucune information fiable n'est disponible, dis-le clairement : "Ce point n'est pas explicitement d√©taill√© dans les volumes consult√©s."
Structure: Contexte ‚Üí M√©canismes ‚Üí Lecture fonctionnelle ‚Üí Int√©gration ‚Üí R√©f√©rences (Volume/section).`,
  model: MODEL,
  tools: [fileSearch],
  modelSettings: { store: true },
});

export async function POST(req: NextRequest) {
  try {
    const { message, chatId, userId } = await req.json();

    // Validation
    if (!message || typeof message !== "string") {
      return NextResponse.json({ error: "Message requis" }, { status: 400 });
    }

    if (!userId || typeof userId !== "string") {
      return NextResponse.json({ error: "userId requis" }, { status: 400 });
    }

    // 1Ô∏è‚É£ Cr√©er ou r√©cup√©rer le Chat
    let currentChatId = chatId;

    if (!currentChatId) {
      // Premi√®re question : cr√©er un nouveau Chat
      const title = message.slice(0, 60) + (message.length > 60 ? "..." : "");
      const newChat = await prisma.chat.create({
        data: {
          userId,
          title,
        },
      });
      currentChatId = newChat.id;
    }

    // 2Ô∏è‚É£ Sauvegarder le message de l'utilisateur
    await prisma.message.create({
      data: {
        chatId: currentChatId,
        role: "user",
        content: message,
      },
    });

    // 3Ô∏è‚É£ Appeler l'assistant RAG
    const conversation: AgentInputItem[] = [
      { role: "user", content: [{ type: "input_text", text: message }] },
    ];

    const runner = new Runner();
    const result = await runner.run(agent, conversation);

    if (!result.finalOutput) {
      return NextResponse.json({ error: "Pas de sortie" }, { status: 500 });
    }

    const reply = result.finalOutput;

    // 4Ô∏è‚É£ Sauvegarder la r√©ponse de l'assistant
    await prisma.message.create({
      data: {
        chatId: currentChatId,
        role: "assistant",
        content: reply,
      },
    });

    // 5Ô∏è‚É£ Mettre √† jour la date du Chat
    await prisma.chat.update({
      where: { id: currentChatId },
      data: { updatedAt: new Date() },
    });

    // 6Ô∏è‚É£ Retourner la r√©ponse + chatId
    return NextResponse.json({
      reply,
      chatId: currentChatId,
    });
  } catch (e: any) {
    console.error("Erreur API:", e);
    return NextResponse.json(
      { error: e?.message ?? "Erreur serveur" },
      { status: 500 }
    );
  }
}

// Healthcheck simple
export async function GET() {
  return NextResponse.json({ ok: true });
}
